---
title: "p8105_hw5_ruijipan"
author: "ruijipan"
date: "2022-11-13"
output: pdf_document
---

## Introduction

This report is used to explain assignment 5 of R language teaching series. Assignment 5 mainly focuses on the study and training of circular grammar in R language.

```{r setup, include=FALSE}
knitr::opts_chunk$set(prompt = TRUE, comment = '', collapse = TRUE, message = FALSE, warning = FALSE)
```


## Problem 1 

First, create a data folder to store the data needed for this report; Under the data folder, create a subfolder named longitudinal-study to store data about a longitudinal study.

The following code shows how to use the list.files function and the `map_dfr` function to read data sets in batches and store the data as a data frame type R language object; Among them, the paste function is used for string merging, which combines the strength of data with data
The file names are merged to form the input parameters of the `read.csv` function.

```{r}
library(tidyverse)
filenames = list.files('./data/longitudinal-study')
filepaths = paste('./data/longitudinal-study/',filenames,sep="")
df = map_dfr(filepaths, read.csv, .id = "input")
df
```

Next, the read data set is cleaned. As you can see, after the data is read in, it is a data frame type object. Rename the input field to arm by using the rename function, and use the character in front of the symbol "_" in the file name as the data content of this field; Then, a new variable "subject_id" is created, and the character after the symbol "_" in the file name is used as the data content of this field. After that, use the select function to rearrange the data fields. As shown in the table below, the data is relatively clean, so no further data cleaning is required.

```{r}
df = rename(df,c(arm=input))
str_arm = strsplit(filenames,"\\.")

var_arm = vector("list", length = 20)
var_id = vector("list", length = 20)
for(i in 1:20){
  var_arm[[i]] = strsplit(str_arm[[i]][1],"_")[[1]][1]
  var_id[[i]] = strsplit(str_arm[[i]][1],"_")[[1]][2]
}
df$arm = var_arm
df$subject_id = var_id
df = df %>%
  select(arm, subject_id, everything())
df
```

## Problem 2

The question 2 uses the murder data collected by The Washington Post. This data set contains a total of 52,179 observations and 12 variables. It mainly includes the ID of the case, the reported date of the case, some basic information of the victim, the place where the case happened and the settlement of the case. The following code creates a new city_state field for this data set, merges the city field and the state field, and calculates the total number of cases and the number of unsolved cases in each city.

```{r}
homicides_df = read.csv("./data/homicides/homicide-data.csv")
homicides_df$city_state = paste(homicides_df$city,homicides_df$state)
head(homicides_df)
```
```{r}
homicidesBycity = 
  homicides_df %>%
  group_by(city) %>%
  summarise(count = n())
head(homicidesBycity)
```

Next, the function prop.test is used to estimate the unsolved proportion of cases in "Baltimore, MD" city (about 65%), and the list is visualized to output the estimated proportion and confidence intervals of the city.

```{r}
unsolved_homicides = 
  homicides_df %>%
  filter(disposition %in% c("Closed without arrest","Open/No arrest")) %>%
  summarise(disposition = "unsolved",total = n())
head(unsolved_homicides)
```
```{r}
Baltimore_MD_unsolved = 
  homicides_df %>%
  filter(disposition %in% c("Closed without arrest","Open/No arrest") & city_state == "Baltimore MD") %>%
  summarise(disposition = "MD_unsolved",total = n())
head(Baltimore_MD_unsolved)
```

```{r}
Baltimore_MD_total = 
  homicides_df %>%
  filter(city_state == "Baltimore MD") %>%
  summarise(disposition = "MD_total",total = n())
head(Baltimore_MD_total)
```
```{r}
prop_test = prop.test(Baltimore_MD_unsolved$total,Baltimore_MD_total$total)
prop_test_df = broom::tidy(prop_test)
prop_test_result_df = prop_test_df %>%
  select(estimate, conf.low, conf.high)
prop_test_result_df
cat("proportion estimate: ",prop_test_df$estimate, "\n")
cat("the 0.95 conf.low: ", prop_test_df$conf.low, "\n")
cat("the 0.95 conf.high: ", prop_test_df$conf.high, "\n")
```

For the above process, we package it into a function, and input it into different cities to get different results.

```{r}
# define a function for the above code
# input: city
# output: prop_test_result_df
proportion = function(x){
  city_total = 
  homicides_df %>%
  filter(city_state == x) %>%
  summarise(disposition = "city_total",total = n())
  
  city_unsolved = 
  homicides_df %>%
  filter(disposition %in% c("Closed without arrest","Open/No arrest") & city_state == x) %>%
  summarise(disposition = "city_unsolved",total = n())
  
  prop_test = prop.test(city_unsolved$total,city_total$total)
  prop_test_df = broom::tidy(prop_test)
  prop_test_result_df = prop_test_df %>%
    select(estimate, conf.low, conf.high)
  
  prop_test_result_df
}
```

After that, we use the map_dfr function to run our packaged functions in batches and output a data frame containing the output results of all cities.

```{r}

prop_result = map_dfr(unique(homicides_df$city_state), proportion, .id = "input")
prop_result = rename(prop_result,c(city_state=input))
prop_result$city_state = unique(homicides_df$city_state)
prop_result

```

For each city, use ggplot to visualize the proportion of its unsolved cases, and its corresponding confidence interval.

```{r, fig.width=10, fig.height=6, fig.align='center'}
prop_result %>%
  ggplot() +
  geom_errorbar(aes(x=city_state, ymin=conf.low, ymax=conf.high,color=city_state), position = position_dodge()) + geom_point(aes(x=city_state, y=estimate,color=city_state)) + 
  ggtitle("unsolved proportion for city_state")
                  
```

## Problem 3

For question 3, use the set.seed function to ensure that our results can be reproduced; Then, the data with mean value of 0 and standard deviation of 5 are generated by cyclic batch. The total number of iterations is 5,000, and each iteration generates 30 observation data.

```{r}
library(tidyverse)
set.seed(1)
data_norm = vector("list", 5000)
for(i in 1:5000){
  data_norm[[i]] = rnorm(n = 30, mean = 0, sd = 5) 
}
listcol_df = 
  tibble(
    sample_id = c(1:5000),
    samp = data_norm
  )

listcol_df = 
  listcol_df %>% 
  mutate(summary = map2(.x = samp, .y = 0, ~t.test(x = .x, mu = .y)))

```

For the above-mentioned generated data, 5000 data sets are tested by single sample mean, and the estimated value and significance are extracted. The confidence level of 0.05 is used to judge whether the test result of each data set is significant, and whether the original hypothesis is rejected or not is stored as a rejected variable.

```{r}
mean_test_result = map_dfr(listcol_df[[3]], broom::tidy, .id = "sample_id") %>%
  select(sample_id, estimate, p.value) %>%
  mutate(rejected = p.value > 0.05)
mean_test_result
```

The above process is packaged into a function, the input parameter of the function is the average value of random numbers with normal distribution, and the output is the test result of each data set.

```{r}
# define a function for the above code
# input: mean
# output: dataframe containing sample_id, true_u, estimate, p.value, rejected
library(tidyverse)
mean_test = function(x){
  set.seed(1)
  data_norm = vector("list", 5000)
  for(i in 1:5000){
    data_norm[[i]] = rnorm(n = 30, mean = x, sd = 5) 
  }
  listcol_df = 
    tibble(
      sample_id = c(1:5000),
      samp = data_norm
    )
  
  listcol_df = 
    listcol_df %>% 
    mutate(summary = map2(.x = samp, .y = x, ~t.test(x = .x, mu = .y)))
  
  mean_test_result = map_dfr(listcol_df[[3]], broom::tidy, .id = "sample_id") %>%
  select(sample_id, estimate, p.value) %>%
  mutate(true_u = x, rejected = p.value > 0.05)
  mean_test_result
}
```

Take the average values of 1, 2, 3, 4, 5, 6, respectively, and run the above functions in batches. The results of the generated functions are stored in the variable mean_test_result_df.

```{r}
mean_vec = c(1,2,3,4,5,6)
mean_test_result_df = map_dfr(mean_vec, mean_test)
```

For the test result list generated above, ggplot is used to show the number of data sets that fail the test under each different mean value. As shown below:

```{r, fig.width=10, fig.height=6, fig.align='center'}
mean_test_result_df %>%
  group_by(true_u) %>%
  summarise(rejected_num = sum(rejected)) %>%
  ggplot(aes(x = true_u, y = rejected_num, fill = true_u)) +
  geom_bar(stat = "identity") + 
  ggtitle("rejected_num for every true_u")
```
Similarly, the list of test results is grouped according to true_u, and the estimated mean value is calculated, and the corresponding scatter plot is drawn.
```{r, fig.width=10, fig.height=6, fig.align='center'}
mean_test_result_df %>%
  group_by(true_u) %>%
  summarise(mean_estimate = mean(estimate)) %>%
  ggplot(aes(x = true_u, y = mean_estimate)) +
  geom_point(shape=1) +
  geom_smooth(method = 'loess') + 
  ggtitle("mean_estimate versus true_u")
```
Group the list of test results according to true_u, first screen out the data that reject the original hypothesis, then calculate the estimated mean and draw the corresponding scatter plot.

```{r, fig.width=10, fig.height=6, fig.align='center'}
mean_test_result_df %>%
  filter(rejected == TRUE) %>%
  group_by(true_u) %>%
  summarise(mean_estimate = mean(estimate)) %>%
  ggplot(aes(x = true_u, y = mean_estimate)) +
  geom_point(shape=1) +
  geom_smooth(method = 'loess') + 
  ggtitle("mean_estimate versus true_u(rejected==TRUE)")
```
It can be found that the estimated mean values calculated by the two scatterplots are almost identical. This is because the percentage of failed tests is very high, reaching 80%~90%. Therefore, the estimated average is of course mainly determined by the data sets that failed tests.
